{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee440825",
   "metadata": {},
   "source": [
    "\n",
    "# Lesson 1 — Tokens & Tokenizers (Build a Tiny BPE)\n",
    "**Goal:** Understand how raw text becomes tokens. You'll implement a *mini* Byte Pair Encoding (BPE) tokenizer and see merges happen.\n",
    "\n",
    "**What you'll learn**\n",
    "- Why LLMs use *subword* tokens\n",
    "- How frequency-based merges reduce token count\n",
    "- How changing the training corpus changes merges and tokenization\n",
    "\n",
    "> ⚙️ Tip: Edit the `data/*.txt` files (e.g., add your own space/dogs/Minecraft text) and re-run!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import collections\n",
    "import re\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "corpus_files = [\"space.txt\", \"animals.txt\", \"minecraft.txt\"]\n",
    "text = \"\"\n",
    "for fname in corpus_files:\n",
    "    text += (data_dir / fname).read_text(encoding=\"utf-8\") + \"\\n\"\n",
    "\n",
    "print(f\"Loaded {len(text)} characters from {corpus_files}\")\n",
    "print(text[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b40b9",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Start from characters\n",
    "We'll implement a very small BPE-like tokenizer:\n",
    "1. Start by splitting words into characters (with a special end-of-word symbol `</w>`).\n",
    "2. Count the frequency of adjacent symbol pairs.\n",
    "3. Merge the most frequent pair into a new symbol.\n",
    "4. Repeat for N merges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vocab(text):\n",
    "    # Split on whitespace; add </w> to mark word boundary\n",
    "    words = re.findall(r\"\\S+\", text.lower())\n",
    "    vocab = collections.Counter([\" \".join(list(w)) + \" </w>\" for w in words])\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    for word, freq in v_in.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        v_out[new_word] = v_out.get(new_word, 0) + freq\n",
    "    return v_out\n",
    "\n",
    "vocab = get_vocab(text)\n",
    "print(f\"Initial vocab size: {len(vocab)} (unique word spellings with char tokens)\")\n",
    "pairs = get_stats(vocab).most_common(10)\n",
    "pairs[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cfb03",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Learn merges\n",
    "Run a small number of merges (e.g., 100). Inspect the most frequent pairs and the final merges list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_merges = 100  # try 50, 100, 200 and compare\n",
    "merges = []\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs: break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    merges.append(best)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "print(f\"Learned {len(merges)} merges. Top 10 merges:\")\n",
    "print(merges[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d738b",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Tokenize new text with the learned merges\n",
    "We'll apply merges greedily from first to last.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(word, merges):\n",
    "    # start from characters + </w>\n",
    "    symbols = list(word.lower()) + [\"</w>\"]\n",
    "    # apply merges in order\n",
    "    for (a,b) in merges:\n",
    "        i = 0\n",
    "        while i < len(symbols)-1:\n",
    "            if symbols[i] == a and symbols[i+1] == b:\n",
    "                symbols[i:i+2] = [a+b]\n",
    "            else:\n",
    "                i += 1\n",
    "    if symbols and symbols[-1] == \"</w>\":\n",
    "        symbols = symbols[:-1]\n",
    "    return symbols\n",
    "\n",
    "samples = [\n",
    "    \"Dogonauts\", \"creeper\", \"astronaut\", \"dogs\", \"minecraft\", \"portal\", \"village\",\n",
    "    \"retriever\", \"falcon\", \"rocketship\"\n",
    "]\n",
    "for s in samples:\n",
    "    print(s, \"->\", tokenize(s, merges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56761b3b",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge\n",
    "- Edit the corpus files and add lots of *Minecraft* words, then re-run the merge training. Do you get different tokens?\n",
    "- Compare token counts for the same sentence under different corpora.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
