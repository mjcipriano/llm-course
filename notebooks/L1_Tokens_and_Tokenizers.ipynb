{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee440825",
   "metadata": {},
   "source": [
    "# Lesson 1 \u2014 Tokens & Tokenizers (Build a Tiny BPE)\n",
    "**Goal:** Understand how raw text becomes machine-friendly tokens. You'll implement a *mini* Byte Pair Encoding (BPE) tokenizer and watch merge rules form in front of you.\n",
    "\n",
    "**Why this matters**\n",
    "- Every LLM reads text as a sequence of integer IDs called **tokens**\u2014similar to how LEGO instructions list part numbers.\n",
    "- A tokenizer translates words, punctuation, and even emojis into those IDs. Different tokenizers create different \"vocabularies.\"\n",
    "- BPE is the technique many modern LLMs (GPT-2/3/4) use because it can flexibly break apart rare words while keeping common chunks whole.\n",
    "\n",
    "**Key idea map**\n",
    "1. Start with characters (like spelling out a word letter-by-letter).\n",
    "2. Find the most common pairs of neighboring characters.\n",
    "3. Glue that pair together into a new symbol, repeat, and build a vocabulary of multi-character pieces.\n",
    "\n",
    "**Vocabulary check**\n",
    "- **Token:** The basic chunk of text an LLM sees (could be a character, word, or syllable-like piece).\n",
    "- **Corpus:** The collection of text you train the tokenizer on. Change the corpus \u2192 change which chunks appear.\n",
    "- **Merge rule:** An instruction that says \"whenever you see `a` next to `n`, treat them as the single token `an`.\"\n",
    "\n",
    "> \u2699\ufe0f Tip: Edit the `data/*.txt` files (e.g., add your own space/dogs/Minecraft stories) and re-run! Changing the training text should change the learned tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import collections\n",
    "import re\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "corpus_files = [\"space.txt\", \"animals.txt\", \"minecraft.txt\"]\n",
    "text = \"\"\n",
    "for fname in corpus_files:\n",
    "    text += (data_dir / fname).read_text(encoding=\"utf-8\") + \"\\n\"\n",
    "\n",
    "print(f\"Loaded {len(text)} characters from {corpus_files}\")\n",
    "print(text[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b40b9",
   "metadata": {},
   "source": [
    "## Step 1: Start from characters\n",
    "We'll implement a very small BPE-like tokenizer:\n",
    "1. **Split words into characters** and tack on a special end-of-word symbol `</w>`. This keeps track of where words stop so that merges don\u2019t jump across spaces.\n",
    "2. **Count the frequency of adjacent symbol pairs.** Think of scanning the text with a magnifying glass and tallying how often each pair shows up.\n",
    "3. **Merge the most frequent pair into a new symbol.** For example, if `t` and `h` appear together constantly, create the new symbol `th`.\n",
    "4. **Repeat the process for N merges.** Each merge teaches the tokenizer a new mini-word. After enough merges, common words become a single token, while rare words stay in smaller pieces.\n",
    "\n",
    "> \ud83d\udcce Analogy: Imagine compressing text by inventing shorthand. If you write \"laugh out loud\" many times, you eventually invent \"lol\". BPE does the same but automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vocab(text):\n",
    "    # Split on whitespace; add </w> to mark word boundary\n",
    "    words = re.findall(r\"\\S+\", text.lower())\n",
    "    vocab = collections.Counter([\" \".join(list(w)) + \" </w>\" for w in words])\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    for word, freq in v_in.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        v_out[new_word] = v_out.get(new_word, 0) + freq\n",
    "    return v_out\n",
    "\n",
    "vocab = get_vocab(text)\n",
    "print(f\"Initial vocab size: {len(vocab)} (unique word spellings with char tokens)\")\n",
    "pairs = get_stats(vocab).most_common(10)\n",
    "pairs[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cfb03",
   "metadata": {},
   "source": [
    "## Step 2: Learn merges\n",
    "Now run a loop for a modest number of merges (try 50\u2013200). After each merge:\n",
    "- Print the pair you merged and its frequency to see *why* it was chosen.\n",
    "- Keep a list of all merges. This is your tokenizer\u2019s \"recipe\" for rebuilding tokens from characters.\n",
    "- Notice how new merges create longer and longer chunks\u2014letters \u2192 syllables \u2192 whole words.\n",
    "\n",
    "**Reflection prompts**\n",
    "- Which words quickly become single tokens? (Usually common words like \"the\" or names that show up repeatedly.)\n",
    "- Which words stay split? (Often rare words or made-up ones like fantasy spells.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_merges = 100  # try 50, 100, 200 and compare\n",
    "merges = []\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs: break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    merges.append(best)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "print(f\"Learned {len(merges)} merges. Top 10 merges:\")\n",
    "print(merges[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d738b",
   "metadata": {},
   "source": [
    "## Step 3: Tokenize new text with the learned merges\n",
    "With your merge list in hand, you can tokenize any new sentence by applying the merges greedily from first to last.\n",
    "\n",
    "**What to observe**\n",
    "- Try tokenizing a sentence that was *not* in the training corpus. Do the tokens still make sense?\n",
    "- Compare the number of tokens before and after training. Fewer tokens for the same sentence usually means your tokenizer captured useful chunks.\n",
    "- Peek at the actual token pieces\u2014you should see familiar letter combinations that reflect the stories you fed it.\n",
    "\n",
    "> \ud83e\uddea Mini-experiment: Tokenize the same sentence using (1) only characters and (2) your trained merges. Count tokens both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(word, merges):\n",
    "    # start from characters + </w>\n",
    "    symbols = list(word.lower()) + [\"</w>\"]\n",
    "    # apply merges in order\n",
    "    for (a,b) in merges:\n",
    "        i = 0\n",
    "        while i < len(symbols)-1:\n",
    "            if symbols[i] == a and symbols[i+1] == b:\n",
    "                symbols[i:i+2] = [a+b]\n",
    "            else:\n",
    "                i += 1\n",
    "    if symbols and symbols[-1] == \"</w>\":\n",
    "        symbols = symbols[:-1]\n",
    "    return symbols\n",
    "\n",
    "samples = [\n",
    "    \"Dogonauts\", \"creeper\", \"astronaut\", \"dogs\", \"minecraft\", \"portal\", \"village\",\n",
    "    \"retriever\", \"falcon\", \"rocketship\"\n",
    "]\n",
    "for s in samples:\n",
    "    print(s, \"->\", tokenize(s, merges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56761b3b",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "- **Corpus remix:** Add lots of Minecraft vocabulary (\"Redstone\", \"Creeper\", \"Netherite\") to the corpus, retrain, and inspect the merge list. Do those words become single tokens?\n",
    "- **Compare corpora:** Train once on a space-themed story and once on an animal-themed story. Tokenize the same sentence with both tokenizers and compare the outputs.\n",
    "- **Stretch goal:** Write a function that visualizes how token counts shrink as you increase the number of merges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "llm-course"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
