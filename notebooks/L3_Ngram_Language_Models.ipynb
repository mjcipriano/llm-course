{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d14f06",
   "metadata": {},
   "source": [
    "# Lesson 3 â€” N-gram Language Models & Cross-Entropy\n",
    "**Goal:** Train unigram, bigram, and trigram language models on your custom corpus; generate sample text; and measure how surprised the model is using cross-entropy.\n",
    "\n",
    "**Where weâ€™re headed**\n",
    "- An **N-gram** model predicts the next token by looking at the previous `N-1` tokens. Example: a trigram (N=3) predicts the next word using the last two words.\n",
    "- By counting how often sequences appear in your text, you can estimate probabilities like `P(\"spaceship\" | \"the\", \"silver\")`.\n",
    "- Once you know those probabilities, you can both *sample* new text and *score* how likely a sentence is.\n",
    "\n",
    "**Vocabulary check**\n",
    "- **Unigram / Bigram / Trigram:** Models that use 0, 1, or 2 previous tokens respectively.\n",
    "- **Conditional probability:** The chance of event A given event B already happened, written `P(A | B)`.\n",
    "- **Add-k smoothing:** A trick for avoiding zero probabilities by pretending you saw every possible sequence a tiny bit (`k`) times.\n",
    "- **Cross-entropy / Perplexity:** Numbers that summarize how well the model predicts real text. Lower is better because the model is less surprised.\n",
    "\n",
    "**Step-by-step roadmap**\n",
    "1. **Prepare the corpus.** Reuse your tokenizer from Lesson 1 or keep things word-based for easier counting.\n",
    "2. **Count N-grams.** Use Python dictionaries or `collections.Counter` to count how often each contextâ†’next-token pair appears.\n",
    "3. **Turn counts into probabilities.** Divide counts by the total count for that context, optionally applying add-k smoothing.\n",
    "4. **Sample text.** Starting from a special start token, repeatedly draw the next token based on the probabilities you learned.\n",
    "5. **Evaluate with cross-entropy.** Feed in a held-out sentence, look up the probability of each token, and compute the average negative log probability.\n",
    "\n",
    "> ðŸ§  Mental model: Imagine predicting the next word in a story. If you just read \"The dragon breathed\", you strongly expect \"fire\". N-gram models capture that instinct by counting real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5995552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import re, random, math, collections\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "text = \"\"\n",
    "for fname in [\"space.txt\",\"animals.txt\",\"minecraft.txt\"]:\n",
    "    text += (data_dir / fname).read_text(encoding=\"utf-8\") + \"\\n\"\n",
    "\n",
    "tokens = re.findall(r\"[a-zA-Z']+|[.,!?;:]\", text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986577b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ngrams(tokens, n):\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        yield tuple(tokens[i:i+n])\n",
    "\n",
    "def train_ngram(tokens, n=2, k=1.0):\n",
    "    counts = collections.Counter(ngrams(tokens, n))\n",
    "    ctx_counts = collections.Counter(ngrams(tokens, n-1)) if n>1 else None\n",
    "    vocab = sorted(set(tokens))\n",
    "    V = len(vocab)\n",
    "    def prob(context, w):\n",
    "        if n == 1:\n",
    "            return (counts[(w,)] + k) / (len(tokens) + k*V)\n",
    "        else:\n",
    "            c = counts[context + (w,)]\n",
    "            ctx = ctx_counts[context]\n",
    "            return (c + k) / (ctx + k*V)\n",
    "    return prob, vocab\n",
    "\n",
    "unigram, V1 = train_ngram(tokens, 1, k=1.0)\n",
    "bigram, V2 = train_ngram(tokens, 2, k=0.5)\n",
    "trigram, V3 = train_ngram(tokens, 3, k=0.1)\n",
    "len(V1), len(V2), len(V3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928235d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "def sample(prob, vocab, n=2, max_len=30, temperature=1.0, seed=None):\n",
    "    random.seed(seed)\n",
    "    result = []\n",
    "    if n == 1:\n",
    "        context = ()\n",
    "    elif n == 2:\n",
    "        context = (random.choice(vocab),)\n",
    "    else:\n",
    "        context = (random.choice(vocab), random.choice(vocab))\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # build distribution\n",
    "        scores = []\n",
    "        for w in vocab:\n",
    "            p = prob(context, w)\n",
    "            scores.append(p ** (1.0/temperature))\n",
    "        s = sum(scores)\n",
    "        r = random.random() * s\n",
    "        cum = 0.0\n",
    "        for w, sc in zip(vocab, scores):\n",
    "            cum += sc\n",
    "            if cum >= r:\n",
    "                result.append(w)\n",
    "                break\n",
    "        # advance context\n",
    "        if n == 1:\n",
    "            context = ()\n",
    "        elif n == 2:\n",
    "            context = (w,)\n",
    "        else:\n",
    "            context = (context[-1], w)\n",
    "    return \" \".join(result)\n",
    "\n",
    "print(\"Bigram sample:\", sample(bigram, V2, n=2, temperature=0.8, seed=42))\n",
    "print(\"Trigram sample:\", sample(trigram, V3, n=3, temperature=0.9, seed=7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy(prob, vocab, tokens, n):\n",
    "    # evaluate on held-out tail\n",
    "    split = int(0.8*len(tokens))\n",
    "    test = tokens[split:]\n",
    "    H = 0.0\n",
    "    count = 0\n",
    "    for i in range(len(test)):\n",
    "        if n == 1:\n",
    "            context = ()\n",
    "        elif n == 2:\n",
    "            if i < 1: continue\n",
    "            context = (test[i-1],)\n",
    "        else:\n",
    "            if i < 2: continue\n",
    "            context = (test[i-2], test[i-1])\n",
    "        p = max(prob(context, test[i]), 1e-12)\n",
    "        H += -math.log2(p)\n",
    "        count += 1\n",
    "    return H / max(count,1)\n",
    "\n",
    "for n, p, V in [(1, unigram, V1),(2, bigram, V2),(3, trigram, V3)]:\n",
    "    H = cross_entropy(p, V, tokens, n)\n",
    "    ppl = 2**H\n",
    "    print(f\"{n}-gram: cross-entropy={H:.2f}, perplexity={ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae57b18",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "- **Smoothing sweeps:** Change the add-k value (0, 0.1, 1.0) and record how perplexity changes on a held-out paragraph.\n",
    "- **Order comparison:** Plot perplexity for unigram vs. bigram vs. trigram. When does adding more context help? When does data sparsity hurt?\n",
    "- **Creative sampling:** Try temperature scaling when sampling to make the output more or less random. Compare to deterministic \"pick the max\" decoding.\n",
    "- **Error detective:** Find examples where the trigram model makes a weird prediction. Can you explain it from the counts it saw?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
