{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d14f06",
   "metadata": {},
   "source": [
    "\n",
    "# Lesson 3 â€” N-gram Language Models & Cross-Entropy\n",
    "**Goal:** Train unigram/bigram/trigram language models on your corpus; sample text; compute cross-entropy.\n",
    "\n",
    "**What you'll learn**\n",
    "- Conditional probabilities P(w_t | w_{t-1}, ...)\n",
    "- Add-k smoothing to avoid zeros\n",
    "- Sampling with temperature\n",
    "- Measuring model quality with cross-entropy/perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5995552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import re, random, math, collections\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "text = \"\"\n",
    "for fname in [\"space.txt\",\"animals.txt\",\"minecraft.txt\"]:\n",
    "    text += (data_dir / fname).read_text(encoding=\"utf-8\") + \"\\n\"\n",
    "\n",
    "tokens = re.findall(r\"[a-zA-Z']+|[.,!?;:]\", text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986577b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ngrams(tokens, n):\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        yield tuple(tokens[i:i+n])\n",
    "\n",
    "def train_ngram(tokens, n=2, k=1.0):\n",
    "    counts = collections.Counter(ngrams(tokens, n))\n",
    "    ctx_counts = collections.Counter(ngrams(tokens, n-1)) if n>1 else None\n",
    "    vocab = sorted(set(tokens))\n",
    "    V = len(vocab)\n",
    "    def prob(context, w):\n",
    "        if n == 1:\n",
    "            return (counts[(w,)] + k) / (len(tokens) + k*V)\n",
    "        else:\n",
    "            c = counts[context + (w,)]\n",
    "            ctx = ctx_counts[context]\n",
    "            return (c + k) / (ctx + k*V)\n",
    "    return prob, vocab\n",
    "\n",
    "unigram, V1 = train_ngram(tokens, 1, k=1.0)\n",
    "bigram, V2 = train_ngram(tokens, 2, k=0.5)\n",
    "trigram, V3 = train_ngram(tokens, 3, k=0.1)\n",
    "len(V1), len(V2), len(V3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928235d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "def sample(prob, vocab, n=2, max_len=30, temperature=1.0, seed=None):\n",
    "    random.seed(seed)\n",
    "    result = []\n",
    "    if n == 1:\n",
    "        context = ()\n",
    "    elif n == 2:\n",
    "        context = (random.choice(vocab),)\n",
    "    else:\n",
    "        context = (random.choice(vocab), random.choice(vocab))\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # build distribution\n",
    "        scores = []\n",
    "        for w in vocab:\n",
    "            p = prob(context, w)\n",
    "            scores.append(p ** (1.0/temperature))\n",
    "        s = sum(scores)\n",
    "        r = random.random() * s\n",
    "        cum = 0.0\n",
    "        for w, sc in zip(vocab, scores):\n",
    "            cum += sc\n",
    "            if cum >= r:\n",
    "                result.append(w)\n",
    "                break\n",
    "        # advance context\n",
    "        if n == 1:\n",
    "            context = ()\n",
    "        elif n == 2:\n",
    "            context = (w,)\n",
    "        else:\n",
    "            context = (context[-1], w)\n",
    "    return \" \".join(result)\n",
    "\n",
    "print(\"Bigram sample:\", sample(bigram, V2, n=2, temperature=0.8, seed=42))\n",
    "print(\"Trigram sample:\", sample(trigram, V3, n=3, temperature=0.9, seed=7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy(prob, vocab, tokens, n):\n",
    "    # evaluate on held-out tail\n",
    "    split = int(0.8*len(tokens))\n",
    "    test = tokens[split:]\n",
    "    H = 0.0\n",
    "    count = 0\n",
    "    for i in range(len(test)):\n",
    "        if n == 1:\n",
    "            context = ()\n",
    "        elif n == 2:\n",
    "            if i < 1: continue\n",
    "            context = (test[i-1],)\n",
    "        else:\n",
    "            if i < 2: continue\n",
    "            context = (test[i-2], test[i-1])\n",
    "        p = max(prob(context, test[i]), 1e-12)\n",
    "        H += -math.log2(p)\n",
    "        count += 1\n",
    "    return H / max(count,1)\n",
    "\n",
    "for n, p, V in [(1, unigram, V1),(2, bigram, V2),(3, trigram, V3)]:\n",
    "    H = cross_entropy(p, V, tokens, n)\n",
    "    ppl = 2**H\n",
    "    print(f\"{n}-gram: cross-entropy={H:.2f}, perplexity={ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae57b18",
   "metadata": {},
   "source": [
    "\n",
    "### Challenges\n",
    "- Change smoothing `k` and see effects on perplexity.\n",
    "- Add more themed text to `data/` and re-train.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
