{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee5f34f",
   "metadata": {},
   "source": [
    "# Lesson 6 — Evaluate, Prompt, and Add Simple Safety\n",
    "**Goal:** Measure model quality (perplexity), practice deliberate prompting, and implement a tiny safety/guardrails layer.\n",
    "\n",
    "**Why this matters**\n",
    "- Building a model is only half the story—you need to know if it performs well and behaves responsibly.\n",
    "- Evaluation metrics like perplexity tell you how shocked the model is by real data. Lower perplexity = better predictions.\n",
    "- Prompt engineering and safety checks help you steer outputs once the model is deployed.\n",
    "\n",
    "**Vocabulary check**\n",
    "- **Perplexity:** `exp(cross_entropy)`. Imagine how many equally likely options the model juggles at each step. Perplexity 5 ≈ \"the model thinks there are about 5 plausible next tokens on average.\"\n",
    "- **Held-out set:** A chunk of text you never showed the model during training, used purely for evaluation.\n",
    "- **Prompt template:** A repeatable structure that gives the model context, instructions, and examples.\n",
    "- **Guardrail / Safety filter:** Rules or models that catch unwanted inputs or outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33465d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math, re\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# We'll reuse Lesson 4's character-level model if available.\n",
    "# Otherwise, demonstrate perplexity with n-gram from Lesson 3 re-implemented quickly.\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "text = \"\"\n",
    "for fname in [\"space.txt\",\"animals.txt\",\"minecraft.txt\"]:\n",
    "    text += (data_dir / fname).read_text(encoding=\"utf-8\") + \"\\n\"\n",
    "tokens = re.findall(r\"[a-zA-Z']+|[.,!?;:]\", text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple bigram LM for evaluation demo\n",
    "import collections, math, random\n",
    "def ngrams(tokens, n):\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        yield tuple(tokens[i:i+n])\n",
    "\n",
    "def train_bigram(tokens, k=0.5):\n",
    "    counts = collections.Counter(ngrams(tokens, 2))\n",
    "    ctx_counts = collections.Counter(ngrams(tokens, 1))\n",
    "    vocab = sorted(set(tokens))\n",
    "    V = len(vocab)\n",
    "    def prob(context, w):\n",
    "        c = counts[(context,w)]\n",
    "        ctx = ctx_counts[(context,)]\n",
    "        return (c + k) / (ctx + k*V)\n",
    "    return prob, vocab\n",
    "\n",
    "bigram, V = train_bigram(tokens, k=0.5)\n",
    "\n",
    "def cross_entropy(prob, vocab, tokens):\n",
    "    split = int(0.8*len(tokens))\n",
    "    test = tokens[split:]\n",
    "    H = 0.0\n",
    "    count = 0\n",
    "    for i in range(1, len(test)):\n",
    "        p = max(prob(test[i-1], test[i]), 1e-12)\n",
    "        H += -math.log2(p)\n",
    "        count += 1\n",
    "    return H/max(count,1), 2**(H/max(count,1))\n",
    "\n",
    "H, ppl = cross_entropy(bigram, V, tokens)\n",
    "print(f\"Bigram perplexity on held-out: {ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb5d6e",
   "metadata": {},
   "source": [
    "## Prompting patterns (for larger LLMs)\n",
    "When you use a bigger model (like GPT-2/3+), structure prompts with:\n",
    "- **Role/Goal:** “You are a helpful math tutor…” (sets the mindset)\n",
    "- **Constraints:** “Use numbered steps, keep answers under 5 sentences.”\n",
    "- **Examples (few-shot):** Provide 1–3 mini demonstrations of the task with correct answers.\n",
    "- **Checks:** “Double-check arithmetic before answering. If unsure, say you’re unsure.”\n",
    "\n",
    "**Practice drill**\n",
    "1. Write a plain prompt for your fine-tuned model (Lesson 5) and note the response.\n",
    "2. Rewrite it using the structure above and compare. Did clarity improve? Did the model stay on topic better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d890f",
   "metadata": {},
   "source": [
    "## Simple Safety Filter (demo)\n",
    "Below is a tiny demonstration of *rule-based* screening (e.g., reject if input matches forbidden patterns). Real systems layer many techniques:\n",
    "- Keyword or regex filters for obviously disallowed content.\n",
    "- Classification models trained to detect safety categories.\n",
    "- Human review for tricky edge cases.\n",
    "\n",
    "**Mini project suggestion**\n",
    "- Extend the provided rules carefully. For example, add patterns for spoilers or personal data.\n",
    "- Log when the filter triggers so you can review decisions later.\n",
    "- Reflect on limitations: rule lists can miss rephrased or subtle unsafe requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263edf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FORBIDDEN = [\n",
    "    r\"how to make a bomb\",\n",
    "    r\"credit card number\",\n",
    "    r\"social security number\",\n",
    "]\n",
    "\n",
    "def safe_input(user_text):\n",
    "    t = user_text.lower()\n",
    "    for pat in FORBIDDEN:\n",
    "        if re.search(pat, t):\n",
    "            return False, f\"Blocked by rule: {pat}\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "tests = [\n",
    "    \"Tell me a Minecraft story about wolves\",\n",
    "    \"how to make a bomb from household items\",\n",
    "    \"What's a credit card number?\"\n",
    "]\n",
    "for t in tests:\n",
    "    ok, msg = safe_input(t)\n",
    "    print(f\"{t!r} -> {ok}, {msg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52598b",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "- **Metric mix:** Evaluate both your trigram model (Lesson 3) and tiny Transformer (Lesson 4) on the same held-out text. Compare perplexity—does the Transformer win?\n",
    "- **Prompt A/B test:** Design two prompt templates for the same task and run them on a bigger LLM. Collect responses and vote on which template works better.\n",
    "- **Safety upgrade:** Add a second layer to the filter (e.g., simple sentiment analysis) and document scenarios it catches or misses.\n",
    "- **Reflection journal:** Write a short summary of what each evaluation number means and how you would explain it to a teammate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
