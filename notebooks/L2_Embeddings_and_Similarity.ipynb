{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968719c2",
   "metadata": {},
   "source": [
    "# Lesson 2 — Embeddings & Similarity (Co-occurrence → SVD)\n",
    "**Goal:** Build *your own* word embeddings from a mini corpus using co-occurrence counts + SVD, then explore nearest neighbors with cosine similarity.\n",
    "\n",
    "**Why this matters**\n",
    "- LLMs don’t understand words by dictionary definitions—they use vectors (lists of numbers) that capture meaning based on context.\n",
    "- Word embeddings are the bridge from raw tokens to math. Similar words should land near each other in vector space.\n",
    "- By building embeddings from scratch you see how \"meaning\" emerges from simple counting.\n",
    "\n",
    "**Vocabulary check**\n",
    "- **Embedding:** A dense vector (e.g., length 16) that represents a word. Distance between vectors tells you how related two words are.\n",
    "- **Co-occurrence window:** Slide a window over the text and count how often words appear next to each other. More shared neighbors → stronger relationship.\n",
    "- **SVD (Singular Value Decomposition):** A matrix factorization trick that compresses big count tables into smaller, informative vectors—like picking the main themes in your text.\n",
    "- **Cosine similarity:** A way to measure how close two vectors point. 1.0 = same direction, 0 = unrelated, -1 = opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import re, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = Path(\"../data\")\n",
    "text = \"\"\n",
    "for fname in [\"space.txt\",\"animals.txt\",\"minecraft.txt\"]:\n",
    "    text += (data_dir / fname).read_text(encoding=\"utf-8\") + \"\\n\"\n",
    "\n",
    "# basic cleanup\n",
    "tokens = re.findall(r\"[a-zA-Z']+\", text.lower())\n",
    "vocab = sorted(set(tokens))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "len(vocab), tokens[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90327a6",
   "metadata": {},
   "source": [
    "## Build the co-occurrence matrix\n",
    "1. Choose a **window size** (start with 2). For each center word, look `window` words to the left and right.\n",
    "2. Count every pair (center, neighbor). This builds a big square matrix where entry `(i, j)` is \"how often word *i* sees word *j*.\"\n",
    "3. Normalize if you want (optional). For a beginner run, raw counts are fine.\n",
    "\n",
    "**Things to notice**\n",
    "- Common words like \"the\" co-occur with almost everything—later we’ll see how SVD tames that.\n",
    "- If you add Minecraft text, words like \"pickaxe\" and \"diamond\" should co-occur frequently and strengthen their connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window = 2\n",
    "V = len(vocab)\n",
    "cooc = np.zeros((V,V), dtype=np.float32)\n",
    "\n",
    "for i, w in enumerate(tokens):\n",
    "    wi = word2idx[w]\n",
    "    for j in range(max(0,i-window), min(len(tokens), i+window+1)):\n",
    "        if j == i: continue\n",
    "        wj = word2idx[tokens[j]]\n",
    "        cooc[wi, wj] += 1.0\n",
    "\n",
    "print(\"Co-occurrence built:\", cooc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffdd9f",
   "metadata": {},
   "source": [
    "## Use SVD to get low-dimensional embeddings\n",
    "1. Feed the co-occurrence matrix into a truncated SVD (e.g., keep the top 16 components).\n",
    "2. The resulting matrix gives you a vector for each word.\n",
    "3. You can plot the first two dimensions to see clusters on a 2D scatter plot.\n",
    "\n",
    "**Intuition booster**\n",
    "- Think of SVD as asking: \"If I could describe each word using only a few secret themes, what would those themes be?\"\n",
    "- Those themes often end up being concepts like fantasy vs. science, animals vs. tools, etc., depending on your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SVD\n",
    "U, S, VT = np.linalg.svd(cooc + 1e-6, full_matrices=False)\n",
    "dims = 16\n",
    "emb = U[:, :dims] * S[:dims]\n",
    "\n",
    "# 2D for plotting\n",
    "emb2 = emb[:, :2]\n",
    "\n",
    "# Pick some words to label (space/dogs/minecraft themed)\n",
    "focus = [w for w in [\"dog\", \"dogs\", \"wolf\", \"wolves\", \"creeper\",\"village\",\"portal\",\n",
    "                     \"star\",\"stars\",\"rocket\",\"ship\",\"moon\",\"falcon\",\"cheetahs\",\"pandas\"]\n",
    "         if w in word2idx]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(emb2[:,0], emb2[:,1], alpha=0.1)\n",
    "for w in focus:\n",
    "    i = word2idx[w]\n",
    "    plt.text(emb2[i,0], emb2[i,1], w)\n",
    "plt.title(\"2D Embeddings (SVD of co-occurrence)\")\n",
    "plt.savefig(\"../images/embeddings_2d.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494847f3",
   "metadata": {},
   "source": [
    "## Cosine similarity & nearest neighbors\n",
    "- Write a helper that computes cosine similarity between any two word vectors.\n",
    "- For a chosen query word, sort all other words by similarity and print the top 5 neighbors.\n",
    "- Also try negative examples: which words are least similar? (They’ll have near-zero or negative cosine.)\n",
    "\n",
    "**Experiment ideas**\n",
    "- Compare neighbors for the same word when trained on two different corpora (space vs. Minecraft).\n",
    "- Pick a word with multiple meanings (\"bank\"). Does your tiny corpus give it one sense or mix them up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa074d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine(a,b):\n",
    "    return float(np.dot(a,b) / (np.linalg.norm(a)*np.linalg.norm(b) + 1e-9))\n",
    "\n",
    "def neighbors(query, k=8):\n",
    "    if query not in word2idx:\n",
    "        return []\n",
    "    qi = word2idx[query]\n",
    "    sims = []\n",
    "    for i in range(len(vocab)):\n",
    "        if i == qi: continue\n",
    "        sims.append((cosine(emb[qi], emb[i]), idx2word[i]))\n",
    "    sims.sort(reverse=True)\n",
    "    return sims[:k]\n",
    "\n",
    "for q in [\"dog\",\"creeper\",\"star\",\"village\"]:\n",
    "    print(q, \"->\", neighbors(q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8264df4",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "- **Window sweep:** Train embeddings with window sizes 1, 2, 4. Chart how neighbor lists change. Smaller windows capture grammar; larger windows capture topics.\n",
    "- **Dimensionality tweak:** Try SVD dimensions of 2, 8, 32. Does the 2D plot lose information compared to 32D cosine neighbors?\n",
    "- **Story mix:** Combine two themed corpora and see if the embedding space separates the themes (e.g., one cluster for space words, another for dogs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
