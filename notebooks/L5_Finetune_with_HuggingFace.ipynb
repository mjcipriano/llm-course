{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5337135",
   "metadata": {},
   "source": [
    "# Lesson 5 â€” Fine-tune a Small Pretrained Model (Hugging Face)\n",
    "**Goal:** Use the `transformers` library to fine-tune a compact GPT-style model on your custom corpus.\n",
    "\n",
    "**How this connects to previous lessons**\n",
    "- Lessons 1â€“4 taught you how to build each component from scratch. Now youâ€™ll stand on the shoulders of a pretrained model and *nudge* it toward your style of text.\n",
    "- Fine-tuning adjusts millions of weights slightly so the model prefers your data (e.g., Minecraft diaries or sci-fi mission logs).\n",
    "\n",
    "**Vocabulary check**\n",
    "- **Pretrained model:** A network already trained on a giant dataset. We just adapt it.\n",
    "- **Checkpoint:** A saved bundle of model weights and configuration.\n",
    "- **Dataset / Data collator:** Objects that feed tokenized text into the trainer with the right shapes.\n",
    "- **Learning rate:** Controls how big each weight update is. Too high â†’ instability, too low â†’ very slow progress.\n",
    "- **Overfitting:** When the model memorizes the training sentences instead of learning their style. Monitor validation loss or sample generations to catch this.\n",
    "\n",
    "**Plan of attack**\n",
    "1. Pick a tiny pretrained model (e.g., `sshleifer/tiny-gpt2`) so it trains quickly on CPU.\n",
    "2. Load or build a tokenizer. You can reuse Hugging Faceâ€™s tokenizer or plug in the one you made earlier.\n",
    "3. Create a dataset from your `.txt` files in `data/`, chunk it into blocks, and feed it into the Trainer API.\n",
    "4. Configure training arguments (epochs, batch size, learning rate) and launch training.\n",
    "5. After fine-tuning, generate text and compare it to the base modelâ€™s outputâ€”does it sound more like your stories?\n",
    "\n",
    "> ðŸ’¡ Tip: Keep runs short at first (e.g., 100â€“300 steps). Quick experiments teach you how hyperparameters affect results without waiting forever.\n",
    "\n",
    "> âš ï¸ This requires internet (to download the model) and a Python environment with `transformers`, `datasets`, and `accelerate`. A CPU will work for a tiny run; GPU is faster.\n",
    "> If you *donâ€™t* have internet, skip to the fallback at the bottom (use the tiny transformer from Lesson 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4076131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, add these dependencies with uv (run in a terminal or a notebook shell cell):\n",
    "# !uv add transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"../data\")\n",
    "corpus_files = [\"space.txt\",\"animals.txt\",\"minecraft.txt\"]\n",
    "text = \"\"\n",
    "for f in corpus_files:\n",
    "    text += (data_dir/f).read_text(encoding=\"utf-8\") + \"\\n\"\n",
    "\n",
    "open(\"../data/all_corpus.txt\",\"w\",encoding=\"utf-8\").write(text)\n",
    "print(\"Prepared all_corpus.txt with length:\", len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf75414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "ds = Dataset.from_dict({\"text\":[open(\"../data/all_corpus.txt\",\"r\",encoding=\"utf-8\").read()]})\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "model_name = \"distilgpt2\"  # tiny, good for demo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize dataset\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "tok_ds = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/distilgpt2_finetune\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=tok_ds, data_collator=collator)\n",
    "trainer.train()\n",
    "trainer.save_model(\"../models/distilgpt2_finetune\")\n",
    "tokenizer.save_pretrained(\"../models/distilgpt2_finetune\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "gen = pipeline(\"text-generation\", model=\"../models/distilgpt2_finetune\", tokenizer=tokenizer)\n",
    "print(gen(\"In the village library\", max_length=50, num_return_sequences=1)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fcf44",
   "metadata": {},
   "source": [
    "## Fallback (no internet)\n",
    "If you don't have internet, reuse the **Lesson 4** tiny Transformer:\n",
    "1. Train it on your base corpus and save checkpoints using `torch.save(model.state_dict(), ...)`.\n",
    "2. When you have new text, reload the weights, continue training for a few more epochs, and sample again.\n",
    "3. Compare outputs before and after the extra training. Even a scratch-built model can be fine-tuned!\n",
    "\n",
    "> ðŸ“¦ Bonus: Package your model by saving both the tokenizer (merge list) and weights so friends can try it on their machines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}