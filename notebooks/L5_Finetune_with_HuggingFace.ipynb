{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5337135",
   "metadata": {},
   "source": [
    "\n",
    "# Lesson 5 — Fine-tune a Small Pretrained Model (Hugging Face)\n",
    "**Goal:** Use `transformers` to fine-tune a compact GPT on your custom corpus.\n",
    "\n",
    "**What you'll learn**\n",
    "- Tokenizer + model loading\n",
    "- Build a dataset from local `.txt` files\n",
    "- Fine-tune for a few hundred steps\n",
    "- Generate text from your fine-tuned model\n",
    "\n",
    "> ⚠️ This requires internet (to download the model) and a Python env with `transformers`, `datasets`, and `accelerate`. A CPU will work for a tiny run; GPU is faster.\n",
    "> If you *don’t* have internet, skip to the fallback at the bottom (use the tiny transformer from Lesson 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4076131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed:\n",
    "# !pip install transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b0741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "data_dir = Path(\"../data\")\n",
    "corpus_files = [\"space.txt\",\"animals.txt\",\"minecraft.txt\"]\n",
    "text = \"\"\n",
    "for f in corpus_files:\n",
    "    text += (data_dir/f).read_text(encoding=\"utf-8\") + \"\\n\"\n",
    "\n",
    "open(\"../data/all_corpus.txt\",\"w\",encoding=\"utf-8\").write(text)\n",
    "print(\"Prepared all_corpus.txt with length:\", len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf75414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "ds = Dataset.from_dict({\"text\":[open(\"../data/all_corpus.txt\",\"r\",encoding=\"utf-8\").read()]})\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "model_name = \"distilgpt2\"  # tiny, good for demo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize dataset\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "tok_ds = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/distilgpt2_finetune\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=tok_ds, data_collator=collator)\n",
    "trainer.train()\n",
    "trainer.save_model(\"../models/distilgpt2_finetune\")\n",
    "tokenizer.save_pretrained(\"../models/distilgpt2_finetune\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "gen = pipeline(\"text-generation\", model=\"../models/distilgpt2_finetune\", tokenizer=tokenizer)\n",
    "print(gen(\"In the village library\", max_length=50, num_return_sequences=1)[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fcf44",
   "metadata": {},
   "source": [
    "\n",
    "## Fallback (no internet)\n",
    "If you don't have internet, use the **Lesson 4** tiny transformer. You can load its weights (if saved) and continue training on new text, or just re-run Lesson 4 with updated `data/*.txt`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
