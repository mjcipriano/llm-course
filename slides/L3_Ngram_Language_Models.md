# L3 Ngram Language Models

- Goal: Next-token prediction with counts and smoothing
- Hands-on: train 1/2/3-gram LMs; sample text; compute cross-entropy
- Key concept: language modeling is probability over sequences
